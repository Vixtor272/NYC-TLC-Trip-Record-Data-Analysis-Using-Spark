{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207b57a3",
   "metadata": {},
   "source": [
    "# Enriquecimiento con Taxi Zones y unificación de Green con Yellow trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd8bb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark inicializado: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# --- Celda 0: inicialización (ejecutar primero) ---\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Paquetes que Spark debe cargar (ajusta versiones si es necesario)\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "    '--packages net.snowflake:spark-snowflake_2.12:3.1.2,'\n",
    "    'net.snowflake:snowflake-jdbc:3.24.2 pyspark-shell'\n",
    ")\n",
    "\n",
    "# Ahora sí importamos pyspark y creamos la sesión\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Evita sesiones múltiples: si existe, detenla y crea una nueva limpia\n",
    "if 'spark' in globals():\n",
    "    try:\n",
    "        spark.stop()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "conf = SparkConf().setAppName(\"NYC_TLC_ingest\").setMaster(\"local[*]\")\n",
    "# opcional: conf.set(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:3.1.2,net.snowflake:snowflake-jdbc:3.24.2\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Comprueba\n",
    "print(\"Spark inicializado:\", spark.version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88930ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfURL: LKVTWCT-PPC14557.snowflakecomputing.com\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"/home/jovyan/work/.env\", override=True)\n",
    "\n",
    "account = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "sf_url = os.getenv(\"SNOWFLAKE_URL\") or account\n",
    "if sf_url and not sf_url.endswith(\"snowflakecomputing.com\"):\n",
    "    sf_url = f\"{sf_url}.snowflakecomputing.com\"\n",
    "\n",
    "if not all([account, os.getenv(\"SNOWFLAKE_USER\"), os.getenv(\"SNOWFLAKE_PASSWORD\")]):\n",
    "    raise RuntimeError(\"Faltan variables de Snowflake en /home/jovyan/work/.env\")\n",
    "\n",
    "print(\"sfURL:\", sf_url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d857b09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.18.0, Python Version: 3.11.6, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:__main__:Verificando/creando tabla TAXI_ZONES...\n",
      "INFO:__main__:write_pandas success=True nchunks=1 rows=265\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import tempfile\n",
    "import logging\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "# --- logging ---\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- credenciales ---\n",
    "load_dotenv()\n",
    "\n",
    "account = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "warehouse = os.getenv(\"SNOWFLAKE_WH\")\n",
    "database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "schema = os.getenv(\"SNOWFLAKE_SCHEMA\", \"BRONZE\")\n",
    "role = os.getenv(\"SNOWFLAKE_ROLE\")\n",
    "\n",
    "# --- validaciones ---\n",
    "if not all([user, password, warehouse, database]):\n",
    "    raise RuntimeError(\"Faltan credenciales Snowflake: revisa SNOWFLAKE_USER/PASSWORD/WH/DATABASE en .env\")\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    logger.warning(\"No se detectó 'spark' en globals(). El flujo usa pandas/write_pandas, no Spark.\")\n",
    "\n",
    "# --- URL CSV ---\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "\n",
    "# --- archivo temporal ---\n",
    "tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "tmp_path = tmp.name\n",
    "tmp.close()\n",
    "\n",
    "def download_file(url, temp_path, timeout=30):\n",
    "    \"\"\"Descarga con manejo de errores\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, stream=True, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        with open(temp_path, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(\"Error descargando %s: %s\", url, e)\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    if not download_file(url, tmp_path):\n",
    "        raise RuntimeError(f\"No se pudo descargar {url}\")\n",
    "\n",
    "    # --- leer CSV ---\n",
    "    df = pd.read_csv(tmp_path, encoding='utf-8')\n",
    "    df.columns = [c.strip().upper() for c in df.columns]\n",
    "\n",
    "    # --- conectar a Snowflake ---\n",
    "    account_for_connector = account\n",
    "    if account and account.endswith(\".snowflakecomputing.com\"):\n",
    "        account_for_connector = account.replace(\".snowflakecomputing.com\", \"\")\n",
    "\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=user,\n",
    "        password=password,\n",
    "        account=account_for_connector,\n",
    "        warehouse=warehouse,\n",
    "        database=database,\n",
    "        schema=schema,\n",
    "        role=role\n",
    "    )\n",
    "\n",
    "    cs = conn.cursor()\n",
    "\n",
    "    # --- Crear tabla si no existe ---\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database}.{schema}.TAXI_ZONES (\n",
    "        LOCATIONID INTEGER,\n",
    "        BOROUGH STRING,\n",
    "        ZONE STRING,\n",
    "        SERVICE_ZONE STRING\n",
    "    );\n",
    "    \"\"\"\n",
    "    logger.info(\"Verificando/creando tabla TAXI_ZONES...\")\n",
    "    cs.execute(create_table_sql)\n",
    "    cs.close()\n",
    "\n",
    "    # --- Cargar datos ---\n",
    "    success, nchunks, nrows, _ = write_pandas(conn, df, 'TAXI_ZONES', database=database, schema=schema)\n",
    "    logger.info(\"write_pandas success=%s nchunks=%s rows=%s\", success, nchunks, nrows)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error en la ejecución: %s\", e)\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "    try:\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"No se pudo borrar temp file %s: %s\", tmp_path, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14d47ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando creación de tabla usando Snowflake Connector...\n",
      "✅ snowflake-connector-python está instalado\n",
      "Conectando a Snowflake...\n",
      "✅ Conexión establecida\n",
      "Ejecutando consulta CREATE TABLE...\n",
      "✅ Consulta ejecutada. Resultado: ('Table ENRICHED_TRIPS successfully created.',)\n",
      "✅ Tabla creada exitosamente usando snowflake-connector-python\n",
      "Verificando tabla con Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 236\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVerificando tabla con Spark...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m check_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnowflake\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msfOptions) \\\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_schema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ENRICHED_TRIPS\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Tabla verificada. Registros: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcheck_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMuestra de los datos:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m check_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Chunk PySpark: unir, normalizar, enriquecer y escribir a Snowflake - CON SNOWFLAKE CONNECTOR\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- SF options ---\n",
    "sfOptions = {\n",
    "    \"sfURL\": os.getenv(\"SNOWFLAKE_URL\") or os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"sfUser\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"sfPassword\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"sfDatabase\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"sfSchema\": os.getenv(\"SNOWFLAKE_SCHEMA\", \"BRONZE\"),\n",
    "    \"sfWarehouse\": os.getenv(\"SNOWFLAKE_WH\"),\n",
    "    \"sfRole\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "}\n",
    "\n",
    "database = sfOptions[\"sfDatabase\"]\n",
    "schema = sfOptions[\"sfSchema\"]\n",
    "target_schema = \"BRONZE\"\n",
    "\n",
    "# Consulta SQL - optimizada para mejor rendimiento\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE \"{database}\".\"{target_schema}\".\"ENRICHED_TRIPS\" AS\n",
    "WITH green AS (\n",
    "    SELECT\n",
    "        NULL AS AIRPORT_FEE,\n",
    "        INGEST_RUN_ID,\n",
    "        INGEST_TIMESTAMP,\n",
    "        YEAR,\n",
    "        MONTH,\n",
    "        VENDORID,\n",
    "        PASSENGER_COUNT,\n",
    "        TRIP_DISTANCE,\n",
    "        PULOCATIONID,\n",
    "        DOLOCATIONID,\n",
    "        RATECODEID,\n",
    "        STORE_AND_FWD_FLAG,\n",
    "        PAYMENT_TYPE,\n",
    "        FARE_AMOUNT,\n",
    "        TIP_AMOUNT,\n",
    "        TOTAL_AMOUNT,\n",
    "        EXTRA,\n",
    "        MTA_TAX,\n",
    "        TOLLS_AMOUNT,\n",
    "        IMPROVEMENT_SURCHARGE,\n",
    "        CONGESTION_SURCHARGE,\n",
    "        CBD_CONGESTION_FEE,\n",
    "        EHAIL_FEE,\n",
    "        TRIP_TYPE,\n",
    "        SERVICE AS SERVICE_TYPE,\n",
    "        LPEP_PICKUP_DATETIME AS PICKUP_DATETIME,\n",
    "        LPEP_DROPOFF_DATETIME AS DROPOFF_DATETIME\n",
    "    FROM \"{database}\".\"{schema}\".\"GREEN_TRIPS\"\n",
    "),\n",
    "\n",
    "yellow AS (\n",
    "    SELECT\n",
    "        AIRPORT_FEE,\n",
    "        INGEST_RUN_ID,\n",
    "        INGEST_TIMESTAMP,\n",
    "        YEAR,\n",
    "        MONTH,\n",
    "        VENDORID,\n",
    "        PASSENGER_COUNT,\n",
    "        TRIP_DISTANCE,\n",
    "        PULOCATIONID,\n",
    "        DOLOCATIONID,\n",
    "        RATECODEID,\n",
    "        STORE_AND_FWD_FLAG,\n",
    "        PAYMENT_TYPE,\n",
    "        FARE_AMOUNT,\n",
    "        TIP_AMOUNT,\n",
    "        TOTAL_AMOUNT,\n",
    "        EXTRA,\n",
    "        MTA_TAX,\n",
    "        TOLLS_AMOUNT,\n",
    "        IMPROVEMENT_SURCHARGE,\n",
    "        CONGESTION_SURCHARGE,\n",
    "        CBD_CONGESTION_FEE,\n",
    "        NULL AS EHAIL_FEE,\n",
    "        NULL AS TRIP_TYPE,\n",
    "        SERVICE AS SERVICE_TYPE,\n",
    "        TPEP_PICKUP_DATETIME AS PICKUP_DATETIME,\n",
    "        TPEP_DROPOFF_DATETIME AS DROPOFF_DATETIME\n",
    "    FROM \"{database}\".\"{schema}\".\"YELLOW_TRIPS\"\n",
    "),\n",
    "\n",
    "unioned_trips AS (\n",
    "    SELECT * FROM green\n",
    "    UNION ALL\n",
    "    SELECT * FROM yellow\n",
    "),\n",
    "\n",
    "standardized_trips AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        CONVERT_TIMEZONE('UTC', 'America/New_York', PICKUP_DATETIME) AS PICKUP_DATETIME_EST,\n",
    "        CONVERT_TIMEZONE('UTC', 'America/New_York', DROPOFF_DATETIME) AS DROPOFF_DATETIME_EST,\n",
    "        \n",
    "        CASE \n",
    "            WHEN PAYMENT_TYPE = 1 THEN 'Credit card'\n",
    "            WHEN PAYMENT_TYPE = 2 THEN 'Cash'\n",
    "            WHEN PAYMENT_TYPE = 3 THEN 'No charge'\n",
    "            WHEN PAYMENT_TYPE = 4 THEN 'Dispute'\n",
    "            WHEN PAYMENT_TYPE = 6 THEN 'Voided trip'\n",
    "            ELSE 'Not specified'\n",
    "        END AS PAYMENT_TYPE_DESC,\n",
    "        \n",
    "        CASE \n",
    "            WHEN RATECODEID = 1 THEN 'Standard rate'\n",
    "            WHEN RATECODEID = 2 THEN 'JFK'\n",
    "            WHEN RATECODEID = 3 THEN 'Newark'\n",
    "            WHEN RATECODEID = 4 THEN 'Nassau or Westchester'\n",
    "            WHEN RATECODEID = 5 THEN 'Negotiated fare'\n",
    "            WHEN RATECODEID = 6 THEN 'Group ride'\n",
    "            ELSE 'Unknown'\n",
    "        END AS RATECODE_DESC,\n",
    "        \n",
    "        CASE \n",
    "            WHEN UPPER(STORE_AND_FWD_FLAG) = 'Y' THEN 'Yes'\n",
    "            WHEN UPPER(STORE_AND_FWD_FLAG) = 'N' THEN 'No'\n",
    "            ELSE 'Unknown'\n",
    "        END AS STORE_AND_FWD_FLAG_DESC\n",
    "    FROM unioned_trips\n",
    "),\n",
    "\n",
    "enriched_with_zones AS (\n",
    "    SELECT\n",
    "        st.*,\n",
    "        pz.zone AS PICKUP_ZONE,\n",
    "        pz.borough AS PICKUP_BOROUGH,\n",
    "        pz.service_zone AS PICKUP_SERVICE_ZONE,\n",
    "        dz.zone AS DROPOFF_ZONE,\n",
    "        dz.borough AS DROPOFF_BOROUGH,\n",
    "        dz.service_zone AS DROPOFF_SERVICE_ZONE\n",
    "    FROM standardized_trips st\n",
    "    LEFT JOIN \"{database}\".\"{schema}\".\"TAXI_ZONES\" pz \n",
    "        ON st.PULOCATIONID = pz.locationid\n",
    "    LEFT JOIN \"{database}\".\"{schema}\".\"TAXI_ZONES\" dz \n",
    "        ON st.DOLOCATIONID = dz.locationid\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    INGEST_RUN_ID,\n",
    "    INGEST_TIMESTAMP,\n",
    "    SERVICE_TYPE,\n",
    "    YEAR,\n",
    "    MONTH,\n",
    "    PICKUP_DATETIME_EST AS PICKUP_DATETIME,\n",
    "    DROPOFF_DATETIME_EST AS DROPOFF_DATETIME,\n",
    "    VENDORID,\n",
    "    PASSENGER_COUNT,\n",
    "    TRIP_DISTANCE,\n",
    "    RATECODEID,\n",
    "    RATECODE_DESC,\n",
    "    STORE_AND_FWD_FLAG_DESC,\n",
    "    PULOCATIONID,\n",
    "    PICKUP_ZONE,\n",
    "    PICKUP_BOROUGH,\n",
    "    PICKUP_SERVICE_ZONE,\n",
    "    DOLOCATIONID, \n",
    "    DROPOFF_ZONE,\n",
    "    DROPOFF_BOROUGH,\n",
    "    DROPOFF_SERVICE_ZONE,\n",
    "    PAYMENT_TYPE,\n",
    "    PAYMENT_TYPE_DESC,\n",
    "    FARE_AMOUNT,\n",
    "    TIP_AMOUNT,\n",
    "    EXTRA,\n",
    "    MTA_TAX,\n",
    "    TOLLS_AMOUNT,\n",
    "    IMPROVEMENT_SURCHARGE,\n",
    "    CONGESTION_SURCHARGE,\n",
    "    CBD_CONGESTION_FEE,\n",
    "    EHAIL_FEE,\n",
    "    TOTAL_AMOUNT,\n",
    "    AIRPORT_FEE,\n",
    "    TRIP_TYPE\n",
    "FROM enriched_with_zones\n",
    "\"\"\"\n",
    "\n",
    "print(\"Ejecutando creación de tabla usando Snowflake Connector...\")\n",
    "\n",
    "try:\n",
    "    # Intentar importar snowflake-connector-python\n",
    "    import snowflake.connector\n",
    "    \n",
    "    print(\"✅ snowflake-connector-python está instalado\")\n",
    "    \n",
    "    # Extraer información de conexión de sfOptions\n",
    "    # El formato de account normalmente es lo que está antes de .snowflakecomputing.com\n",
    "    account = sfOptions['sfURL']\n",
    "    if '.snowflakecomputing.com' in account:\n",
    "        account = account.split('.snowflakecomputing.com')[0]\n",
    "    \n",
    "    # Conectar directamente a Snowflake\n",
    "    print(\"Conectando a Snowflake...\")\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=sfOptions['sfUser'],\n",
    "        password=sfOptions['sfPassword'],\n",
    "        account=account,\n",
    "        warehouse=sfOptions['sfWarehouse'],\n",
    "        database=sfOptions['sfDatabase'],\n",
    "        schema=sfOptions['sfSchema'],\n",
    "        role=sfOptions['sfRole']\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Conexión establecida\")\n",
    "    \n",
    "    # Crear cursor\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Ejecutar la consulta\n",
    "    print(\"Ejecutando consulta CREATE TABLE...\")\n",
    "    cursor.execute(create_table_query)\n",
    "    \n",
    "    # Obtener resultado\n",
    "    result = cursor.fetchone()\n",
    "    print(f\"✅ Consulta ejecutada. Resultado: {result}\")\n",
    "    \n",
    "    # Cerrar conexión\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"✅ Tabla creada exitosamente usando snowflake-connector-python\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"snowflake-connector-python no está instalado\")\n",
    "    print(\"Instálalo con: pip install snowflake-connector-python\")\n",
    "    \n",
    "    # Fallback a la opción de Spark\n",
    "    print(\"Intentando con Spark SQL como fallback...\")\n",
    "    try:\n",
    "        spark.sql(create_table_query)\n",
    "        print(\"✅ Tabla creada exitosamente con Spark SQL\")\n",
    "        \n",
    "        # Verificar\n",
    "        check_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**sfOptions) \\\n",
    "            .option(\"dbtable\", f\"{database}.{target_schema}.ENRICHED_TRIPS\") \\\n",
    "            .load()\n",
    "        \n",
    "        print(f\"✅ Tabla verificada. Registros: {check_df.count()}\")\n",
    "        check_df.show(5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error con Spark SQL: {str(e)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error con snowflake-connector: {str(e)}\")\n",
    "    \n",
    "    # Intentar con una versión más simple para diagnóstico\n",
    "    print(\"Intentando con consulta simplificada para diagnóstico...\")\n",
    "    \n",
    "    try:\n",
    "        import snowflake.connector\n",
    "        \n",
    "        account = sfOptions['sfURL']\n",
    "        if '.snowflakecomputing.com' in account:\n",
    "            account = account.split('.snowflakecomputing.com')[0]\n",
    "        \n",
    "        conn = snowflake.connector.connect(\n",
    "            user=sfOptions['sfUser'],\n",
    "            password=sfOptions['sfPassword'],\n",
    "            account=account,\n",
    "            warehouse=sfOptions['sfWarehouse'],\n",
    "            database=sfOptions['sfDatabase'],\n",
    "            schema=sfOptions['sfSchema'],\n",
    "            role=sfOptions['sfRole']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Consulta de prueba simple\n",
    "        test_query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE \"{database}\".\"{target_schema}\".\"ENRICHED_TRIPS_TEST\" AS\n",
    "        SELECT \n",
    "            'test' as source,\n",
    "            COUNT(*) as total_records\n",
    "        FROM \"{database}\".\"{schema}\".\"GREEN_TRIPS\"\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(test_query)\n",
    "        result = cursor.fetchone()\n",
    "        print(f\"✅ Prueba exitosa. Resultado: {result}\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Error en prueba: {str(e2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
